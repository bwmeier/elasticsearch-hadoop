description = 'Elasticsearch Hadoop'

buildscript {
    repositories {
        maven { url 'http://repo.springsource.org/plugins-release' }
        maven { url 'http://dl.bintray.com/content/aalmiray/asciidoctor' }
        maven { url 'http://jcenter.bintray.com' }
    }
    dependencies {
        classpath('org.springframework.build.gradle:propdeps-plugin:0.0.4')
		classpath('org.asciidoctor:asciidoctor-gradle-plugin:0.5.0')
    }
}

repositories {
  mavenCentral()
  maven { url 'https://repository.cloudera.com/artifactory/cloudera-repos' }
  maven { url "http://oss.sonatype.org/content/groups/public/" }
  // JDO ec2 missing from Maven Central
  maven { url "http://www.datanucleus.org/downloads/maven2" }
  maven { url "http://conjars.org/repo" }
}

apply plugin: "java"
apply plugin: 'eclipse'
apply plugin: 'idea'
apply from: "$rootDir/maven.gradle"
apply plugin: 'propdeps'
apply plugin: 'propdeps-idea'
apply plugin: 'propdeps-eclipse'
apply plugin: 'asciidoctor'
// report plugins
apply plugin: 'findbugs'
apply plugin: 'pmd'
apply plugin: 'jacoco'

def List hadoop = []
def hadoopDistro = project.hasProperty("distro") ? project.getProperty("distro") : "hadoopStable"

// Hadoop aliases
def hadoopStableVersion = hadoop12Version
def hadoopYarnVersion = hadoop2Version
def hadoopVersion = hadoopStableVersion 

switch (hadoopDistro) {

  // Hadoop YARN/2.0.x
  case "hadoopYarn":
    hadoopVersion = hadoopYarnVersion
    println "Using Apache Hadoop YARN [$hadoopVersion]"
    hadoop = ["org.apache.hadoop:hadoop-client:$hadoopVersion"]

    break;
  // Hadoop 1.2.x
  case "hadoop12":
    hadoopVersion = hadoop12Version
    println "Using Apache Hadoop 1.2.x [$hadoopVersion]"
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;
	
  // Hadoop 1.1.x
  case "hadoop11":
    hadoopVersion = hadoop11Version
    println "Using Apache Hadoop 1.1.x [$hadoopVersion]"
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;

  // Hadoop 1.0.x
  case "hadoop10":
    hadoopVersion = hadoop10Version
    println "Using Apache Hadoop 1.0.x [$hadoopVersion]"
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"]

    break;

  default:
    println "Using Apache Hadoop Stable [$hadoopVersion]"
    hadoopVersion = hadoopStableVersion
    hadoop = ["org.apache.hadoop:hadoop-streaming:$hadoopVersion",
              "org.apache.hadoop:hadoop-tools:$hadoopVersion"] 
}

dependencies {
	provided(hadoop)
	
	provided("org.codehaus.jackson:jackson-mapper-asl:$jacksonVersion")
    testRuntime("log4j:log4j:$log4jVersion")

    // Pig
    optional("org.apache.pig:pig:$pigVersion")

	ext.hiveGroup = "org.apache.hive"
    // Hive
    optional("$hiveGroup:hive-service:$hiveVersion")
	
	// Cascading
	optional("cascading:cascading-hadoop:$cascadingVersion")
	optional("cascading:cascading-local:$cascadingVersion")
    
    // needed by the Hive Server tests - removed since Hive 0.11
    // testRuntime "$hiveGroup:hive-builtins:$hiveVersion"

    // Testing
    testCompile "junit:junit:$junitVersion"
	testCompile "org.hamcrest:hamcrest-all:$hamcrestVersion"
    testCompile "org.elasticsearch:elasticsearch:$esVersion"
	
	if (hadoopVersion.contains("1.0.")) {
		// missing dependency in Hadoop 1.0.3/1.0.4
		testCompile "commons-io:commons-io:2.1"
	}
	
	// Required by Pig
	testRuntime "com.google.guava:guava:11.0"
	testRuntime "dk.brics.automaton:automaton:1.11-8"
	optional("joda-time:joda-time:$jodaVersion")
	
	// Required by Hive + Pig
	// testRuntime "org.antlr:antlr-runtime:$antlrVersion"
	
	// Required by Hive
	testRuntime "org.apache.hive:hive-jdbc:$hiveVersion"
}

configurations.all {
  resolutionStrategy {
	forcedModules = ['commons-httpclient:commons-httpclient:3.0.1']
  }
}

compileJava {
    sourceCompatibility = 1.6
    targetCompatibility = 1.6 

    options.compilerArgs << "-Xlint:unchecked"
    //options.compilerArgs << "-Xlint:deprecation"
	options.compilerArgs << "-Xlint:options"
}

ext.skipCascading = true
ext.skipMR = true
ext.skipHive = true
ext.skipPig = true

task enableMRTests {
    description = "Enable Map/Reduce tests"
    group = "Verification"
    doLast() {
        project.ext.skipMR = false
   }
}

task enableCascadingTests {
    description = "Enable Cascading tests"
    group = "Verification"
    doLast() {
        project.ext.skipCascading = false
   }
}

task enableHiveTests {
    description = "Enable Hive tests"
    group = "Verification"
    doLast() {
        project.ext.skipHive = false
  }
}

task enablePigTests {
    description = "Enable Pig tests"
    group = "Verification"
    
    doLast() {
        project.ext.skipPig = false
  }
}

task enableIntegrationTests() {
    description = "Enable integration tests"
    group = "Verification"
    doLast() {
      println "Enable all integration tests"
	  enableMRTests.execute()
  	  enableCascadingTests.execute()
	  enablePigTests.execute()
	  //enableHiveTests.execute()
    }
}

test {
    systemProperties['input.path'] = 'build/classes/test/input'
    systemProperties['output.path'] = 'build/classes/test/output'
    includes = ["org/elasticsearch/hadoop/serialization/*.class", 
				"org/elasticsearch/hadoop/pig/*.class",
				"org/elasticsearch/hadoop/rest/*.class",
				"org/elasticsearch/hadoop/util/**/*.class", 
				"org/elasticsearch/hadoop/integration/**/*Suite.class"]

	minHeapSize = "256m"
	maxHeapSize = "768m"

    testLogging {
        events "started" //, "standardOut", "standardError"
        minGranularity 2
        maxGranularity 2
    }
	
    doFirst() {
        ext.msg = ""

        if (skipMR) {
            ext.msg += "MapReduce "
            excludes.add("**/integration/mr/**")
        }
        if (skipCascading) {
            ext.msg += "Cascading "
            excludes.add("**/integration/cascading/**")
        }
        if (skipHive) {
            ext.msg += "Hive "
            excludes.add("**/integration/hive/**")
        }
        if (skipPig) {
            ext.msg += "Pig"
            excludes.add("**/integration/pig/**")
        }

        if (!msg.isEmpty())
            println "Skipping [$msg] Tests";
	}
	
	jacoco {
        def jacocoEnabled = project.hasProperty("jacocoCoverage")
		enabled = jacocoEnabled
       
        if (jacocoEnabled)
            println "Enabling Jacoco Coverage"
    }
}

javadoc {
	ext.srcDir = file("${projectDir}/docs/src/api")
	

	configure(options) {
		//stylesheetFile = file("${rootProject.projectDir}/docs/src/api/javadoc.css")
		//overview = "${rootProject.projectDir}/docs/src/api/overview.html"
		docFilesSubDirs = true
		outputLevel = org.gradle.external.javadoc.JavadocOutputLevel.QUIET
		breakIterator = true
		author = true
		showFromProtected()

		groups = [
		'Elasticsearch Map/Reduce' : ['org.elasticsearch.hadoop.mr*'],
		'Elasticsearch Cascading' : ['org.elasticsearch.hadoop.cascading*'],
		'Elasticsearch Hive' : ['org.elasticsearch.hadoop.hive*'],
		'Elasticsearch Pig' : ['org.elasticsearch.hadoop.pig*'],
		]

		links = [
			"http://download.oracle.com/javase/6/docs/api",
			"http://commons.apache.org/proper/commons-logging/apidocs/",
			"http://hadoop.apache.org/common/docs/r1.1.2/api/",
			//"http://hbase.apache.org/apidocs/",
			"http://pig.apache.org/docs/r0.12.0/api/",
			"http://hive.apache.org/docs/r0.12.0/api/",
			"https://builds.apache.org/job/Thrift/javadoc/",
			"http://docs.cascading.org/cascading/2.1/javadoc/"
		]

		excludes = [
			"org/elasticsearch/hadoop/rest/**", 
			"org/elasticsearch/hadoop/serialization/**", 
			"org/elasticsearch/hadoop/util/**"
		]
	}

	title = "${rootProject.description} ${version} API"
}

task sourcesJar(type: Jar, dependsOn:classes) {
    classifier = 'sources'
    from sourceSets.main.allJava
}

task javadocJar(type: Jar) {
    classifier = 'javadoc'
    from javadoc
}


// jar used for testing Hadoop remotely (es-hadoop + tests)
task hadoopTestingJar(type: Jar) {
	classifier = 'testing'
	from sourceSets.test.output
	from sourceSets.main.output
}

jar {
    manifest.attributes["Created-By"] = "${System.getProperty("java.version")} (${System.getProperty("java.specification.vendor")})"
    manifest.attributes['Implementation-Title'] = 'elasticsearch-hadoop'
    manifest.attributes['Implementation-Version'] = project.version
    manifest.attributes['Implementation-URL'] = "http://github.com/elasticsearch/elasticsearch-hadoop"
    manifest.attributes['Implementation-Vendor'] = "Elasticsearch"
    manifest.attributes['Implementation-Vendor-Id'] = "org.elasticsearch.hadoop"
    
    def build = System.env['ESHDP.BUILD']
    if (build != null)
        manifest.attributes['Build'] = build
    
    String rev = "unknown"
    
    // parse the git files to find out the revision
    File gitHead = file('.git/HEAD')
    if (gitHead.exists()) {
        gitHead = file('.git/' + gitHead.text.trim().replace('ref: ',''))
        if (gitHead.exists()) { rev = gitHead.text }
    }

    from("$rootDir/docs/src/info") {
        include "license.txt"
        include "notice.txt"
        into "META-INF"
        expand(copyright: new Date().format('yyyy'), version: project.version)
    }

    manifest.attributes['Repository-Revision'] = rev
	
	if (hadoopDistro == "hadoopYarn") {
		classifier = 'yarn'
	}
}

// this is in just as a convenience - the official docs are generated through an external process
asciidoctor {
	//logDocuments = true
	sourceDir = file("docs/src/reference/asciidoc")
	sourceDocumentName = file("docs/src/reference/asciidoc/index.adoc")
    options = [
		doctype: 'book',
        attributes: [
			'source-highlighter': 'highlightjs',
			icons: 'font',
			badges: '',
			toc2: '',
			sectanchors: '',
			//stylesheet: 'github.css',
			copycss: ''
			//'linkcss!': ''
        ]
    ]
}

// reporting

findbugsMain {
  ignoreFailures = true
  effort = "max"
  reportLevel = "low"
  
  reports {
    xml.enabled = true
    html.enabled = false
  }
}

pmdMain {
  ignoreFailures = true
  reports {
    xml.enabled = true
    html.enabled = true
  }
}

jacoco {
	toolVersion = "0.6.3.201306030806"
}

jacocoTestReport {
	reports {
		xml.enabled true
		csv.enabled true
		html.enabled true
	}
}

findbugsTest.enabled = false
pmdTest.enabled = false   

// packaging

task distZip(type: Zip, dependsOn: [jar, sourcesJar, javadocJar]) {
    group = "Distribution"
    //classifier = "dist"
    description = "Builds -${classifier} archive, containing all jars and docs, suitable for download page."

    ext.baseDir = "${project.name}-${project.version}";

    from(".") {
        include "README.md"
        include "LICENSE.txt"
        include "NOTICE.txt"
        into "${baseDir}"
        //expand(yyyy: new Date().format("yyyy"), version: project.version)
    }

    from("build/asciidoc") {
        into "${baseDir}/docs"
    }

    into ("${baseDir}/dist") {
        from "build/libs"
		exclude "*-testing.jar"
    }
}

artifacts {
    archives sourcesJar
    archives javadocJar

	// include YARN is available
	ext.yarn = file("$libsDir/$name-$version-yarn.jar")
	if (yarn.exists()) {
		archives yarn
	}
}

task wrapper(type: Wrapper) {
    description = 'Generates gradlew[.bat] scripts'
    gradleVersion = '1.8'
}

// skip creation of javadoc jars 
assemble.dependsOn = ['jar', 'hadoopTestingJar']
defaultTasks 'build'
